{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61b5452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "\n",
    "import dgl\n",
    "from dgl.data import citation_graph, rdf, knowledge_graph\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "import dgl.function as fn\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.data.utils import _get_dgl_url, download, save_graphs, load_graphs, \\\n",
    "    generate_mask_tensor, idx2mask\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418eba3d",
   "metadata": {},
   "source": [
    "### UTILS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69710d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    dgl.seed(seed)\n",
    "    \n",
    "    \n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Calculate accuracy\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) correct label\n",
    "    :return: float accuracy\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.argmax(logits, dim=1) == labels).item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def micro_macro_f1_score(logits, labels):\n",
    "    \"\"\"Calculate Micro-F1 and Macro-F1 scores\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) \n",
    "    Macro-average precision score can be defined as the arithmetic mean of all the precision scores of different classes.\n",
    "    \"\"\"\n",
    "    prediction = torch.argmax(logits, dim=1).long().numpy()\n",
    "    labels = labels.numpy()\n",
    "    micro_f1 = f1_score(labels, prediction, average='micro')\n",
    "    macro_f1 = f1_score(labels, prediction, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "def split_idx(samples, train_size, val_size, random_state=None):\n",
    "    \"\"\"The samples are divided into training set, test set and validation set, which must be satisfied (represented by floating point numbers):\n",
    "    * 0 < train_size < 1\n",
    "    * 0 < val_size < 1\n",
    "    * train_size + val_size < 1\n",
    "    \"\"\"\n",
    "    train, val = train_test_split(samples, train_size=train_size, random_state=random_state)\n",
    "    if isinstance(val_size, float):\n",
    "        val_size *= len(samples) / len(val)\n",
    "    val, test = train_test_split(val, train_size=val_size, random_state=random_state)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee558c",
   "metadata": {},
   "source": [
    "### GET THE ACM DATASET AND CONVERT TO A DGL GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e87dd",
   "metadata": {},
   "source": [
    "The basic DGL dataset for creating graph datasets. This class defines a basic template class for DGL Dataset. The following steps will be executed automatically:\n",
    "\n",
    "Check whether there is a dataset cache on disk (already processed and stored on the disk) by invoking has_cache(). If true, goto 5.\n",
    "\n",
    "1-Call download() to download the data if url is not None.\n",
    "\n",
    "2-Call process() to process the data.\n",
    "\n",
    "3-Call save() to save the processed dataset on disk and goto 6.\n",
    "\n",
    "4-Call load() to load the processed dataset from disk.\n",
    "\n",
    "Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e0a10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACMDataset(DGLDataset):\n",
    "    \"\"\"ACM dataset, only one heterogeneous graph\n",
    "    Statistical data\n",
    "    -----\n",
    "    * Apex: 17351 author, 4025 paper, 72 field\n",
    "    * Sides: 13407 paper-author, 4025 paper-field\n",
    "    * Number of categories: 3\n",
    "    * paper vertex division: 808 train, 401 valid, 2816 test\n",
    "    Attributes\n",
    "    -----\n",
    "    * num_classes: number of classes\n",
    "    * metapaths: metapaths to use\n",
    "    * predict_ntype: predict vertex type\n",
    "    paper vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(4025, 1903) bag-of-words representation of keywords\n",
    "    * label: tensor(4025)\n",
    "    * train_mask, val_mask, test_mask: tensor(4025)\n",
    "    author vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(17351, 1903) average of associated paper features\n",
    "    field vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(72, 72) one-hot encoding \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        #Get DGL online url for download.\n",
    "        super().__init__('ACM', _get_dgl_url('dataset/ACM.mat'))\n",
    "\n",
    "    def download(self):\n",
    "        file_path = os.path.join(self.raw_dir, 'ACM.mat')\n",
    "        if not os.path.exists(file_path):\n",
    "            download(self.url, path=file_path)\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['paper'].data[k] = self.g.nodes['paper'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        data = sio.loadmat(os.path.join(self.raw_dir, 'ACM.mat'))\n",
    "        p_vs_l = data['PvsL']  # paper-field\n",
    "        p_vs_a = data['PvsA']  # paper-author\n",
    "        p_vs_t = data['PvsT']  # paper-term, bag of words\n",
    "        p_vs_c = data['PvsC']  # paper-conference, labels come from that\n",
    "\n",
    "        # We assign\n",
    "        # (1) KDD papers as class 0 (data mining),\n",
    "        # (2) SIGMOD and VLDB papers as class 1 (database),\n",
    "        # (3) SIGCOMM and MobiCOMM papers as class 2 (communication)\n",
    "        conf_ids = [0, 1, 9, 10, 13]\n",
    "        label_ids = [0, 1, 2, 2, 1]\n",
    "\n",
    "        p_vs_c_filter = p_vs_c[:, conf_ids]\n",
    "        #get indeces of all papers\n",
    "        p_selected = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]\n",
    "        p_vs_l = p_vs_l[p_selected]\n",
    "        p_vs_a = p_vs_a[p_selected]\n",
    "        p_vs_t = p_vs_t[p_selected]\n",
    "        p_vs_c = p_vs_c[p_selected]\n",
    "        \n",
    "        #building the graph\n",
    "        self.g = dgl.heterograph({\n",
    "            ('paper', 'pa', 'author'): p_vs_a.nonzero(),\n",
    "            ('author', 'ap', 'paper'): p_vs_a.transpose().nonzero(),\n",
    "            ('paper', 'pf', 'field'): p_vs_l.nonzero(),\n",
    "            ('field', 'fp', 'paper'): p_vs_l.transpose().nonzero()\n",
    "        })\n",
    "        #the features of a paper are the bag of words associated to a paper\n",
    "        paper_features = torch.FloatTensor(p_vs_t.toarray())  # (4025, 1903)\n",
    "        \n",
    "        #get indces and labels\n",
    "        pc_p, pc_c = p_vs_c.nonzero()\n",
    "        paper_labels = np.zeros(len(p_selected), dtype=np.int64)\n",
    "        for conf_id, label_id in zip(conf_ids, label_ids):\n",
    "            paper_labels[pc_p[pc_c == conf_id]] = label_id\n",
    "        paper_labels = torch.from_numpy(paper_labels)\n",
    "\n",
    "        float_mask = np.zeros(len(pc_p))\n",
    "        for conf_id in conf_ids:\n",
    "            pc_c_mask = (pc_c == conf_id)\n",
    "            float_mask[pc_c_mask] = np.random.permutation(np.linspace(0, 1, pc_c_mask.sum()))\n",
    "        train_idx = np.where(float_mask <= 0.2)[0]\n",
    "        val_idx = np.where((float_mask > 0.2) & (float_mask <= 0.3))[0]\n",
    "        test_idx = np.where(float_mask > 0.3)[0]\n",
    "\n",
    "        num_paper_nodes = self.g.num_nodes('paper')\n",
    "        train_mask = generate_mask_tensor(idx2mask(train_idx, num_paper_nodes))\n",
    "        val_mask = generate_mask_tensor(idx2mask(val_idx, num_paper_nodes))\n",
    "        test_mask = generate_mask_tensor(idx2mask(test_idx, num_paper_nodes))\n",
    "\n",
    "        self.g.nodes['paper'].data['feat'] = paper_features\n",
    "        self.g.nodes['paper'].data['label'] = paper_labels\n",
    "        self.g.nodes['paper'].data['train_mask'] = train_mask\n",
    "        self.g.nodes['paper'].data['val_mask'] = val_mask\n",
    "        self.g.nodes['paper'].data['test_mask'] = test_mask\n",
    "        # The feature of the author vertex is the average of the features of its associated paper vertex\n",
    "        self.g.multi_update_all({'pa': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat'))}, 'sum')\n",
    "        self.g.nodes['field'].data['feat'] = torch.eye(self.g.num_nodes('field'))\n",
    "\n",
    "    def has_cache(self):\n",
    "        return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['pa', 'ap'], ['pf', 'fp']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'paper'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb504b",
   "metadata": {},
   "source": [
    "#### IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "235a37f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      color      director_name  num_critic_for_reviews  duration  \\\n",
      "0     Color      James Cameron                   723.0     178.0   \n",
      "1     Color     Gore Verbinski                   302.0     169.0   \n",
      "2     Color         Sam Mendes                   602.0     148.0   \n",
      "3     Color  Christopher Nolan                   813.0     164.0   \n",
      "4       NaN        Doug Walker                     NaN       NaN   \n",
      "...     ...                ...                     ...       ...   \n",
      "4927  Color       Edward Burns                    14.0      95.0   \n",
      "4928  Color        Scott Smith                     1.0      87.0   \n",
      "4929  Color   Benjamin Roberds                    13.0      76.0   \n",
      "4930  Color        Daniel Hsia                    14.0     100.0   \n",
      "4931  Color           Jon Gunn                    43.0      90.0   \n",
      "\n",
      "      director_facebook_likes  actor_3_facebook_likes        actor_2_name  \\\n",
      "0                         0.0                   855.0    Joel David Moore   \n",
      "1                       563.0                  1000.0       Orlando Bloom   \n",
      "2                         0.0                   161.0        Rory Kinnear   \n",
      "3                     22000.0                 23000.0      Christian Bale   \n",
      "4                       131.0                     NaN          Rob Walker   \n",
      "...                       ...                     ...                 ...   \n",
      "4927                      0.0                   133.0  Caitlin FitzGerald   \n",
      "4928                      2.0                   318.0       Daphne Zuniga   \n",
      "4929                      0.0                     0.0       Maxwell Moody   \n",
      "4930                      0.0                   489.0       Daniel Henney   \n",
      "4931                     16.0                    16.0    Brian Herzlinger   \n",
      "\n",
      "      actor_1_facebook_likes        gross                           genres  \\\n",
      "0                     1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi   \n",
      "1                    40000.0  309404152.0         Action|Adventure|Fantasy   \n",
      "2                    11000.0  200074175.0        Action|Adventure|Thriller   \n",
      "3                    27000.0  448130642.0                  Action|Thriller   \n",
      "4                      131.0          NaN                      Documentary   \n",
      "...                      ...          ...                              ...   \n",
      "4927                   296.0       4584.0                     Comedy|Drama   \n",
      "4928                   637.0          NaN                     Comedy|Drama   \n",
      "4929                     0.0          NaN            Drama|Horror|Thriller   \n",
      "4930                   946.0      10443.0             Comedy|Drama|Romance   \n",
      "4931                    86.0      85222.0                      Documentary   \n",
      "\n",
      "      ... num_user_for_reviews language  country  content_rating       budget  \\\n",
      "0     ...               3054.0  English      USA           PG-13  237000000.0   \n",
      "1     ...               1238.0  English      USA           PG-13  300000000.0   \n",
      "2     ...                994.0  English       UK           PG-13  245000000.0   \n",
      "3     ...               2701.0  English      USA           PG-13  250000000.0   \n",
      "4     ...                  NaN      NaN      NaN             NaN          NaN   \n",
      "...   ...                  ...      ...      ...             ...          ...   \n",
      "4927  ...                 14.0  English      USA       Not Rated       9000.0   \n",
      "4928  ...                  6.0  English   Canada             NaN          NaN   \n",
      "4929  ...                  3.0  English      USA             NaN       1400.0   \n",
      "4930  ...                  9.0  English      USA           PG-13          NaN   \n",
      "4931  ...                 84.0  English      USA              PG       1100.0   \n",
      "\n",
      "      title_year actor_2_facebook_likes imdb_score  aspect_ratio  \\\n",
      "0         2009.0                  936.0        7.9          1.78   \n",
      "1         2007.0                 5000.0        7.1          2.35   \n",
      "2         2015.0                  393.0        6.8          2.35   \n",
      "3         2012.0                23000.0        8.5          2.35   \n",
      "4            NaN                   12.0        7.1           NaN   \n",
      "...          ...                    ...        ...           ...   \n",
      "4927      2011.0                  205.0        6.4           NaN   \n",
      "4928      2013.0                  470.0        7.7           NaN   \n",
      "4929      2013.0                    0.0        6.3           NaN   \n",
      "4930      2012.0                  719.0        6.3          2.35   \n",
      "4931      2004.0                   23.0        6.6          1.85   \n",
      "\n",
      "     movie_facebook_likes  \n",
      "0                   33000  \n",
      "1                       0  \n",
      "2                   85000  \n",
      "3                  164000  \n",
      "4                       0  \n",
      "...                   ...  \n",
      "4927                  413  \n",
      "4928                   84  \n",
      "4929                   16  \n",
      "4930                  660  \n",
      "4931                  456  \n",
      "\n",
      "[4932 rows x 28 columns]\n",
      "       num_critic_for_reviews     duration  director_facebook_likes  \\\n",
      "count             4888.000000  4919.000000              4932.000000   \n",
      "mean               142.711538   108.159382               687.449311   \n",
      "std                121.617362    22.578583              2815.213712   \n",
      "min                  1.000000     7.000000                 0.000000   \n",
      "25%                 53.000000    94.000000                 7.000000   \n",
      "50%                112.000000   104.000000                49.000000   \n",
      "75%                197.000000   118.000000               195.500000   \n",
      "max                813.000000   330.000000             23000.000000   \n",
      "\n",
      "       actor_3_facebook_likes  actor_1_facebook_likes         gross  \\\n",
      "count              4919.00000             4932.000000  4.152000e+03   \n",
      "mean                651.20553             6668.287713  4.854980e+07   \n",
      "std                1681.08616            15150.446402  6.848194e+07   \n",
      "min                   0.00000                0.000000  1.620000e+02   \n",
      "25%                 133.00000              618.000000  5.377819e+06   \n",
      "50%                 372.00000              991.500000  2.556371e+07   \n",
      "75%                 637.00000            11000.000000  6.234110e+07   \n",
      "max               23000.00000           640000.000000  7.605058e+08   \n",
      "\n",
      "       num_voted_users  cast_total_facebook_likes  facenumber_in_poster  \\\n",
      "count     4.932000e+03                4932.000000           4919.000000   \n",
      "mean      8.491599e+04                9854.138686              1.365725   \n",
      "std       1.396285e+05               18326.222891              2.013732   \n",
      "min       5.000000e+00                   0.000000              0.000000   \n",
      "25%       8.998000e+03                1430.750000              0.000000   \n",
      "50%       3.504150e+04                3133.000000              1.000000   \n",
      "75%       9.799350e+04               14030.000000              2.000000   \n",
      "max       1.689764e+06              656730.000000             43.000000   \n",
      "\n",
      "       num_user_for_reviews        budget   title_year  \\\n",
      "count           4914.000000  4.537000e+03  4928.000000   \n",
      "mean             276.882173  3.986040e+07  2002.462256   \n",
      "std              380.777408  2.064230e+08    12.480022   \n",
      "min                1.000000  2.180000e+02  1916.000000   \n",
      "25%               68.000000  6.000000e+06  1999.000000   \n",
      "50%              160.000000  2.000000e+07  2005.000000   \n",
      "75%              331.750000  4.500000e+07  2011.000000   \n",
      "max             5060.000000  1.221550e+10  2016.000000   \n",
      "\n",
      "       actor_2_facebook_likes   imdb_score  aspect_ratio  movie_facebook_likes  \n",
      "count             4928.000000  4932.000000   4625.000000           4932.000000  \n",
      "mean              1675.850852     6.418289      2.129250           7598.097932  \n",
      "std               4080.170643     1.115312      0.789375          19459.191923  \n",
      "min                  0.000000     1.600000      1.180000              0.000000  \n",
      "25%                281.750000     5.800000      1.850000              0.000000  \n",
      "50%                598.000000     6.500000      2.350000            165.500000  \n",
      "75%                920.000000     7.200000      2.350000           3000.000000  \n",
      "max             137000.000000     9.500000     16.000000         349000.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Jhy1993/HAN/master/data/imdb/movie_metadata.csv', encoding='utf8') \\\n",
    "            .dropna(axis=0, subset=['actor_1_name', 'director_name']).reset_index(drop=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86780e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(DGLDataset):\n",
    "    \"\"\"IMDb movie dataset, only one heterogeneous graph\n",
    "    Statistical data\n",
    "    -----\n",
    "    * Apex: 4278 movies, 5257 actors, 2081 directors\n",
    "    * Sides: 12828 movie-actor, 4278 movie-director\n",
    "    * Number of categories: 3\n",
    "    * Movie vertex division: 400 train, 400 valid, 3478 test\n",
    "    Attributes\n",
    "    -----\n",
    "    * num_classes: number of classes\n",
    "    * metapaths: metapaths to use\n",
    "    * predict_ntype: predict vertex type\n",
    "    movie vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(4278, 1299) bag-of-words representation of plot keywords\n",
    "    * label: tensor(4278) 0: Action, 1: Comedy, 2: Drama\n",
    "    * train_mask, val_mask, test_mask: tensor(4278)\n",
    "    actor vertex attributes\n",
    "    -----\n",
    "    * feat: tensor(5257, 1299) average of associated movie features\n",
    "    director vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(2081, 1299) average of associated movie features\n",
    "    \"\"\"\n",
    "    _url = 'https://raw.githubusercontent.com/Jhy1993/HAN/master/data/imdb/movie_metadata.csv'\n",
    "    _seed = 42\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('imdb', self._url)\n",
    "\n",
    "    def download(self):\n",
    "        file_path = os.path.join(self.raw_dir, 'imdb.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            download(self.url, path=file_path)\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['movie'].data[k] = self.g.nodes['movie'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(os.path.join(self.raw_dir, 'imdb.csv'), encoding='utf8') \\\n",
    "            .dropna(axis=0, subset=['actor_1_name', 'director_name']).reset_index(drop=True)\n",
    "        self.labels = self._extract_labels()\n",
    "        self.movies = list(sorted(m.strip() for m in self.data['movie_title']))\n",
    "        self.directors = list(sorted(set(self.data['director_name'])))\n",
    "        self.actors = list(sorted(set(itertools.chain.from_iterable(\n",
    "            self.data[c].dropna().to_list()\n",
    "            for c in ('actor_1_name', 'actor_2_name', 'actor_3_name')\n",
    "        ))))\n",
    "        self.g = self._build_graph()\n",
    "        self._add_ndata()\n",
    "        return self.data\n",
    "\n",
    "    def _extract_labels(self):\n",
    "        labels = np.full(len(self.data), -1)\n",
    "        for i, genres in self.data['genres'].iteritems():\n",
    "            for genre in genres.split('|'):\n",
    "                if genre == 'Action':\n",
    "                    labels[i] = 0\n",
    "                    break\n",
    "                elif genre == 'Comedy':\n",
    "                    labels[i] = 1\n",
    "                    break\n",
    "                elif genre == 'Drama':\n",
    "                    labels[i] = 2\n",
    "                    break\n",
    "        other_idx = np.where(labels == -1)[0]\n",
    "        self.data = self.data.drop(other_idx).reset_index(drop=True)\n",
    "        return np.delete(labels, other_idx, axis=0)\n",
    "\n",
    "    def _build_graph(self):\n",
    "        ma, md = set(), set()\n",
    "        for m, row in self.data.iterrows():\n",
    "            d = self.directors.index(row['director_name'])\n",
    "            md.add((m, d))\n",
    "            for c in ('actor_1_name', 'actor_2_name', 'actor_3_name'):\n",
    "                if row[c] in self.actors:\n",
    "                    a = self.actors.index(row[c])\n",
    "                    ma.add((m, a))\n",
    "        ma, md = list(ma), list(md)\n",
    "        ma_m, ma_a = [e[0] for e in ma], [e[1] for e in ma]\n",
    "        md_m, md_d = [e[0] for e in md], [e[1] for e in md]\n",
    "        return dgl.heterograph({\n",
    "            ('movie', 'ma', 'actor'): (ma_m, ma_a),\n",
    "            ('actor', 'am', 'movie'): (ma_a, ma_m),\n",
    "            ('movie', 'md', 'director'): (md_m, md_d),\n",
    "            ('director', 'dm', 'movie'): (md_d, md_m)\n",
    "        })\n",
    "\n",
    "    def _add_ndata(self):\n",
    "        vectorizer = CountVectorizer(min_df=5)\n",
    "        features = vectorizer.fit_transform(self.data['plot_keywords'].fillna('').values)\n",
    "        self.g.nodes['movie'].data['feat'] = torch.from_numpy(features.toarray()).float()\n",
    "        self.g.nodes['movie'].data['label'] = torch.from_numpy(self.labels).long()\n",
    "\n",
    "        # Actor and director vertex features are the average of their associated movie vertex features\n",
    "        self.g.multi_update_all({\n",
    "            'ma': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "            'md': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat'))\n",
    "        }, 'sum')\n",
    "\n",
    "        n_movies = len(self.movies)\n",
    "        train_idx, val_idx, test_idx = split_idx(np.arange(n_movies), 400, 400, self._seed)\n",
    "        self.g.nodes['movie'].data['train_mask'] = generate_mask_tensor(idx2mask(train_idx, n_movies))\n",
    "        self.g.nodes['movie'].data['val_mask'] = generate_mask_tensor(idx2mask(val_idx, n_movies))\n",
    "        self.g.nodes['movie'].data['test_mask'] = generate_mask_tensor(idx2mask(test_idx, n_movies))\n",
    "\n",
    "    def has_cache(self):\n",
    "        return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['ma', 'am'], ['md', 'dm']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "662715d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\"\"\"Heterogeneous Graph Transformer (HGT)\n",
    "论文链接：https://arxiv.org/pdf/2003.01332\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.ops import edge_softmax\n",
    "from dgl.utils import expand_as_pair\n",
    "\n",
    "\n",
    "class HGTAttention(nn.Module):\n",
    "    \"\"\"HGT attention module\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param k_linear: nn.Linear(d_in, d_out)\n",
    "        :param q_linear: nn.Linear(d_in, d_out)\n",
    "        :param v_linear: nn.Linear(d_in, d_out)\n",
    "        :param w_att: tensor(K, d_out/K, d_out/K)\n",
    "        :param w_msg: tensor(K, d_out/K, d_out/K)\n",
    "        :param mu: tensor(1)\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, out_dim, num_heads, k_linear, q_linear, v_linear, w_att, w_msg, mu):\n",
    "   \n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = out_dim // num_heads\n",
    "        self.k_linear = k_linear\n",
    "        self.q_linear = q_linear\n",
    "        self.v_linear = v_linear\n",
    "        self.w_att = w_att\n",
    "        self.w_msg = w_msg\n",
    "        self.mu = mu\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph bipartite graph (contains only one relation)\n",
    "        :param feat: tensor(N_src, d_in) or (tensor(N_src, d_in), tensor(N_dst, d_in)) input feature\n",
    "        :return: tensor(N_dst, d_out) The target vertex's representation of the relationship\n",
    "        \"\"\"\n",
    "        \n",
    "        #avoids changing the graph features when exiting the function.\n",
    "        with g.local_scope():\n",
    "            feat_src, feat_dst = expand_as_pair(feat, g)\n",
    "            # (N_src, d_in) -> (N_src, d_out) -> (N_src, K, d_out/K)\n",
    "            k = self.k_linear(feat_src).view(-1, self.num_heads, self.d_k)\n",
    "            v = self.v_linear(feat_src).view(-1, self.num_heads, self.d_k)\n",
    "            q = self.q_linear(feat_dst).view(-1, self.num_heads, self.d_k)\n",
    "\n",
    "            # k[:, h] @= w_att[h] => k[n, h, j] = ∑(i) k[n, h, i] * w_att[h, i, j]\n",
    "            k = torch.einsum('nhi,hij->nhj', k, self.w_att)\n",
    "            v = torch.einsum('nhi,hij->nhj', v, self.w_msg)\n",
    "\n",
    "            g.srcdata.update({'k': k, 'v': v})\n",
    "            g.dstdata['q'] = q\n",
    "            g.apply_edges(fn.v_dot_u('q', 'k', 't'))  # g.edata['t']: (E, K, 1)\n",
    "            attn = g.edata.pop('t').squeeze(dim=-1) * self.mu / math.sqrt(self.d_k)\n",
    "            attn = edge_softmax(g, attn)  # (E, K)\n",
    "            g.edata['t'] = attn.unsqueeze(dim=-1)  # (E, K, 1)\n",
    "\n",
    "            g.update_all(fn.u_mul_e('v', 't', 'm'), fn.sum('m', 'h'))\n",
    "            out = g.dstdata['h'].view(-1, self.out_dim)  # (N_dst, d_out)\n",
    "            return out\n",
    "\n",
    "\n",
    "class HGTLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, num_heads, ntypes, etypes, dropout=0.2, use_norm=True):\n",
    "        \"\"\"HGT layer\n",
    "        :param in_dim: int input feature dimension\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param ntypes: List[str] list of vertex types\n",
    "        :param etypes: List[(str, str, str)] list of canonical edge types\n",
    "        :param dropout: dropout: float, optional Dropout probability, default is 0.2\n",
    "        :param use_norm: bool, optional whether to use layer normalization, the default is True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_k = out_dim // num_heads\n",
    "        k_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        q_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        v_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        w_att = {r[1]: nn.Parameter(torch.Tensor(num_heads, d_k, d_k)) for r in etypes}\n",
    "        w_msg = {r[1]: nn.Parameter(torch.Tensor(num_heads, d_k, d_k)) for r in etypes}\n",
    "        mu = {r[1]: nn.Parameter(torch.ones(num_heads)) for r in etypes}\n",
    "        self.reset_parameters(w_att, w_msg)\n",
    "        self.conv = HeteroGraphConv({\n",
    "            etype: HGTAttention(\n",
    "                out_dim, num_heads, k_linear[stype], q_linear[dtype], v_linear[stype],\n",
    "                w_att[etype], w_msg[etype], mu[etype]\n",
    "            ) for stype, etype, dtype in etypes\n",
    "        }, 'mean')\n",
    "\n",
    "        self.a_linear = nn.ModuleDict({ntype: nn.Linear(out_dim, out_dim) for ntype in ntypes})\n",
    "        self.skip = nn.ParameterDict({ntype: nn.Parameter(torch.ones(1)) for ntype in ntypes})\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.use_norm = use_norm\n",
    "        if use_norm:\n",
    "            self.norms = nn.ModuleDict({ntype: nn.LayerNorm(out_dim) for ntype in ntypes})\n",
    "\n",
    "    def reset_parameters(self, w_att, w_msg):\n",
    "        for etype in w_att:\n",
    "            nn.init.xavier_uniform_(w_att[etype])\n",
    "            nn.init.xavier_uniform_(w_msg[etype])\n",
    "\n",
    "    def forward(self, g, feats):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph heterogeneous graph\n",
    "        :param feats: Dict[str, tensor(N_i, d_in)] mapping of vertex types to input vertex features\n",
    "        :return: Dict[str, tensor(N_i, d_out)] mapping of vertex types to output features\n",
    "        \"\"\"\n",
    "        if g.is_block:\n",
    "            feats_dst = {ntype: feats[ntype][:g.num_dst_nodes(ntype)] for ntype in feats}\n",
    "        else:\n",
    "            feats_dst = feats\n",
    "        with g.local_scope():\n",
    "            # STEP 1 --> Heterogeneous Mutual Attention + Heterogeneous Messaging + Goal-Related Aggregation\n",
    "            hs = self.conv(g, (feats, feats))  # {ntype: tensor(N_i, d_out)}\n",
    "\n",
    "            # Residual connections\n",
    "            out_feats = {}\n",
    "            for ntype in g.dsttypes:\n",
    "                if g.num_dst_nodes(ntype) == 0:\n",
    "                    continue\n",
    "                alpha = torch.sigmoid(self.skip[ntype])\n",
    "                trans_out = self.drop(self.a_linear[ntype](hs[ntype]))\n",
    "                out = alpha * trans_out + (1 - alpha) * feats_dst[ntype]\n",
    "                out_feats[ntype] = self.norms[ntype](out) if self.use_norm else out\n",
    "            return out_feats\n",
    "\n",
    "\n",
    "class HGT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, in_dims, hidden_dim, out_dim, num_heads, ntypes, etypes,\n",
    "            predict_ntype, num_layers, dropout=0.2, use_norm=True):\n",
    "        \"\"\"HGT model\n",
    "        :param in_dims: Dict[str, int] mapping of vertex types to input feature dimensions\n",
    "        :param hidden_dim: int hidden feature dimension\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param ntypes: List[str] list of vertex types\n",
    "        :param etypes: List[(str, str, str)] list of canonical edge types\n",
    "        :param predict_ntype: str The type of vertex to be predicted\n",
    "        :param num_layers: int number of layers\n",
    "        :param dropout: dropout: float, optional Dropout probability, default is 0.2\n",
    "        :param use_norm: bool, optional whether to use layer normalization, the default is True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.predict_ntype = predict_ntype\n",
    "        self.adapt_fcs = nn.ModuleDict({\n",
    "            ntype: nn.Linear(in_dim, hidden_dim) for ntype, in_dim in in_dims.items()\n",
    "        })\n",
    "        #create the HGT layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            HGTLayer(hidden_dim, hidden_dim, num_heads, ntypes, etypes, dropout, use_norm)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.predict = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, g, feats):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph heterogeneous graph\n",
    "        :param feats: Dict[str, tensor(N_i, d_in)] mapping of vertex types to input vertex features\n",
    "        :return: tensor(N_i, d_out) The final embedding of the vertex to be predicted\n",
    "        \"\"\"\n",
    "        hs = {ntype: F.gelu(self.adapt_fcs[ntype](feats[ntype])) for ntype in feats}\n",
    "        for layer in self.layers:\n",
    "            hs = layer(g, hs)  # {ntype: tensor(N_i, d_hid)}\n",
    "        out = self.predict(hs[self.predict_ntype])  # tensor(N_i, d_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56799372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss 1.2534 | Train Micro-F1 0.2748 | Train Macro-F1 0.2160 | Val Micro-F1 0.3092 | Val Macro-F1 0.2302 | Test Micro-F1 0.2887 | Test Macro-F1 0.2123\n",
      "Epoch 1 | Train Loss 1.1769 | Train Micro-F1 0.3403 | Train Macro-F1 0.2744 | Val Micro-F1 0.4439 | Val Macro-F1 0.3341 | Test Micro-F1 0.4059 | Test Macro-F1 0.2992\n",
      "Epoch 2 | Train Loss 1.1105 | Train Micro-F1 0.4022 | Train Macro-F1 0.3232 | Val Micro-F1 0.5037 | Val Macro-F1 0.2656 | Test Micro-F1 0.4979 | Test Macro-F1 0.2525\n",
      "Epoch 3 | Train Loss 1.0201 | Train Micro-F1 0.4963 | Train Macro-F1 0.3581 | Val Micro-F1 0.5112 | Val Macro-F1 0.2776 | Test Micro-F1 0.5107 | Test Macro-F1 0.2647\n",
      "Epoch 4 | Train Loss 0.9565 | Train Micro-F1 0.5644 | Train Macro-F1 0.4157 | Val Micro-F1 0.5686 | Val Macro-F1 0.3904 | Test Micro-F1 0.5710 | Test Macro-F1 0.3885\n",
      "Epoch 5 | Train Loss 0.8944 | Train Micro-F1 0.5804 | Train Macro-F1 0.4441 | Val Micro-F1 0.6384 | Val Macro-F1 0.5366 | Test Micro-F1 0.6584 | Test Macro-F1 0.5621\n",
      "Epoch 6 | Train Loss 0.6762 | Train Micro-F1 0.7488 | Train Macro-F1 0.7066 | Val Micro-F1 0.8204 | Val Macro-F1 0.7982 | Test Micro-F1 0.8327 | Test Macro-F1 0.8153\n",
      "Epoch 7 | Train Loss 0.5319 | Train Micro-F1 0.8899 | Train Macro-F1 0.8833 | Val Micro-F1 0.8304 | Val Macro-F1 0.8033 | Test Micro-F1 0.8335 | Test Macro-F1 0.8109\n",
      "Epoch 8 | Train Loss 0.3915 | Train Micro-F1 0.9183 | Train Macro-F1 0.9122 | Val Micro-F1 0.8678 | Val Macro-F1 0.8616 | Test Micro-F1 0.8903 | Test Macro-F1 0.8881\n",
      "Epoch 9 | Train Loss 0.2401 | Train Micro-F1 0.9604 | Train Macro-F1 0.9597 | Val Micro-F1 0.8579 | Val Macro-F1 0.8622 | Test Micro-F1 0.8825 | Test Macro-F1 0.8828\n",
      "Epoch 10 | Train Loss 0.1661 | Train Micro-F1 0.9666 | Train Macro-F1 0.9656 | Val Micro-F1 0.8828 | Val Macro-F1 0.8785 | Test Micro-F1 0.8913 | Test Macro-F1 0.8886\n",
      "Epoch 11 | Train Loss 0.0936 | Train Micro-F1 0.9777 | Train Macro-F1 0.9778 | Val Micro-F1 0.9027 | Val Macro-F1 0.9014 | Test Micro-F1 0.9119 | Test Macro-F1 0.9123\n",
      "Epoch 12 | Train Loss 0.0383 | Train Micro-F1 0.9963 | Train Macro-F1 0.9963 | Val Micro-F1 0.9027 | Val Macro-F1 0.9019 | Test Micro-F1 0.9144 | Test Macro-F1 0.9146\n",
      "Epoch 13 | Train Loss 0.0317 | Train Micro-F1 0.9938 | Train Macro-F1 0.9939 | Val Micro-F1 0.9077 | Val Macro-F1 0.9028 | Test Micro-F1 0.9070 | Test Macro-F1 0.9052\n",
      "Epoch 14 | Train Loss 0.0168 | Train Micro-F1 0.9975 | Train Macro-F1 0.9975 | Val Micro-F1 0.9052 | Val Macro-F1 0.8992 | Test Micro-F1 0.9084 | Test Macro-F1 0.9056\n",
      "Epoch 15 | Train Loss 0.0106 | Train Micro-F1 0.9988 | Train Macro-F1 0.9988 | Val Micro-F1 0.9127 | Val Macro-F1 0.9082 | Test Micro-F1 0.9098 | Test Macro-F1 0.9074\n",
      "Epoch 16 | Train Loss 0.0063 | Train Micro-F1 1.0000 | Train Macro-F1 1.0000 | Val Micro-F1 0.9127 | Val Macro-F1 0.9087 | Test Micro-F1 0.9148 | Test Macro-F1 0.9137\n",
      "Epoch 17 | Train Loss 0.0041 | Train Micro-F1 1.0000 | Train Macro-F1 1.0000 | Val Micro-F1 0.9152 | Val Macro-F1 0.9143 | Test Micro-F1 0.9155 | Test Macro-F1 0.9155\n",
      "Epoch 18 | Train Loss 0.0026 | Train Micro-F1 1.0000 | Train Macro-F1 1.0000 | Val Micro-F1 0.9127 | Val Macro-F1 0.9124 | Test Micro-F1 0.9205 | Test Macro-F1 0.9212\n",
      "Epoch 19 | Train Loss 0.0028 | Train Micro-F1 1.0000 | Train Macro-F1 1.0000 | Val Micro-F1 0.9127 | Val Macro-F1 0.9128 | Test Micro-F1 0.9226 | Test Macro-F1 0.9237\n",
      "Test Micro-F1 0.9226 | Test Macro-F1 0.9237\n",
      "la predizione è :\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
      "        2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1,\n",
      "        2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 2, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2,\n",
      "        2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "DATASET = {\n",
    "    'acm': ACMDataset(),\n",
    "    'imdb': IMDbDataset()\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    set_random_seed(1)\n",
    "    data = DATASET['acm']\n",
    "    g = data[0]\n",
    "    #node type to predict\n",
    "    predict_ntype = data.predict_ntype\n",
    "    #dictionary containing node type and features\n",
    "    features = {ntype: g.nodes[ntype].data['feat'] for ntype in g.ntypes}\n",
    "    #lists containing labels, train-mask,val_mask and test-mask\n",
    "    labels = g.nodes[predict_ntype].data['label']\n",
    "    train_mask = g.nodes[predict_ntype].data['train_mask']\n",
    "    val_mask = g.nodes[predict_ntype].data['val_mask']\n",
    "    test_mask = g.nodes[predict_ntype].data['test_mask']\n",
    "    \n",
    "    #initialization of HGT model\n",
    "    model = HGT(\n",
    "        {ntype: g.nodes[ntype].data['feat'].shape[1] for ntype in g.ntypes},\n",
    "        256, data.num_classes, 8, g.ntypes, g.canonical_etypes,\n",
    "        predict_ntype, 2, 0.5\n",
    "    )\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 1e-3, total_steps=30)\n",
    "    metrics = 'Epoch {:d} | Train Loss {:.4f} | Train Micro-F1 {:.4f} | Train Macro-F1 {:.4f}' \\\n",
    "              ' | Val Micro-F1 {:.4f} | Val Macro-F1 {:.4f}' \\\n",
    "              ' | Test Micro-F1 {:.4f} | Test Macro-F1 {:.4f}'\n",
    "    warnings.filterwarnings('ignore', 'Setting attributes on ParameterDict is not supported')\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        # forward propagation by using all nodes\n",
    "        logits = model(g, features)\n",
    "        #compute loss\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask]) \n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #Total norm of the parameter gradients (viewed as a single vector).\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_scores = micro_macro_f1_score(logits[train_mask], labels[train_mask])\n",
    "        val_scores = evaluate(model, g, features, labels, val_mask, micro_macro_f1_score)\n",
    "        test_scores = evaluate(model, g, features, labels, test_mask, micro_macro_f1_score)\n",
    "        print(metrics.format(epoch, loss.item(), *train_scores, *val_scores, *test_scores))\n",
    "    test_scores = evaluate(model, g, features, labels, test_mask, micro_macro_f1_score)\n",
    "    print('Test Micro-F1 {:.4f} | Test Macro-F1 {:.4f}'.format(*test_scores))\n",
    "    l = logits[test_mask]\n",
    "    pred = l.argmax(1)\n",
    "    print('la predizione è :')\n",
    "    print(pred)\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, g, features, labels, mask, score):\n",
    "    model.eval()\n",
    "    logits = model(g, features)\n",
    "    return score(logits[mask], labels[mask])\n",
    "\n",
    "\n",
    "def main():\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee244f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cbd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
